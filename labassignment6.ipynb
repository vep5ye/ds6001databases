{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment 6: Creating and Connecting to Databases\n",
    "## DS 6001\n",
    "\n",
    "### Instructions\n",
    "Please answer the following questions as completely as possible using text, code, and the results of code as needed. Format your answers in a Jupyter notebook. To receive full credit, make sure you address every part of the problem, and make sure your document is formatted in a clean and professional way.\n",
    "\n",
    "**Please note: you will not be able to use Rivanna for this lab as Rivanna is not set up to work with Docker or with Databases. Problem 1 will guide you through the steps to get databases running on your local system. This can be tricky, so if you have questions or would like some help, please let me know.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 0\n",
    "Import the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import sqlite3\n",
    "import psycopg\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "import pymongo\n",
    "from bson.json_util import loads, dumps\n",
    "import dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "Many database systems run as external software on a computer. Some of the software is commercial (Oracle, Microsoft SQL Server), with limited free use and expensive enterprise level licenses. Other database software is entirely free and open source (MySQL, Postgres, SQlite, MongoDB). Regardless of whether the software is free or not, databases are notoriously difficult to install and start using. More than other kinds of software, database systems seem to run into many errors that are specific to the operating system of your computer. We used to ask students to install Postgres, MySQL, and Mongo, and it was a nightmare because everyone hit some sort of roadblock and those roadblocks were different for every student.\n",
    "\n",
    "That's just how databases seem to be! Not just for students, but for everyone!\n",
    "\n",
    "A growing standard practice in industry is to use **containers** to deploy database systems. Please read the [discussion of containers in the course textbook],(https://jkropko.github.io/surfing-the-data-pipeline/docker.html) if you haven't yet done so to familiarize yourself with the purpose and uses of containers and Docker.\n",
    "\n",
    "This lab will guide you through the installation steps for the software you need to run database systems on your computer (problem 1), build a relational schema (problem 2), document those databases (problem 3), and connect to them through Python with (fingers crossed) as few problems as possible (problem 4 for relational databases, problem 5 for a NoSQL database).\n",
    "\n",
    "With the exception of SQLite (which operates as a Python package), database systems run as external software that must be installed and run on your computer. To make the installation steps easier, you will need some configuration files that I wrote and saved in a GitHub repository. Open your terminal and then type\n",
    "```\n",
    "git clone https://github.com/jkropko/ds6001databases\n",
    "```\n",
    "If this command works, it will create a new directory within your current folder called \"ds6001databases\".\n",
    "\n",
    "* Check that this folder exists and contains the following files: LICENSE, README.md, compose.yaml, and requirements.txt. (Don't worry if there are a couple extra files.)\n",
    "\n",
    "* To make things easier, save the notebook file you will be using for your Lab 6 work inside the \"ds6001databases\" folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a\n",
    "After cloning the ds6001databases repository, open and examine the file called compose.yaml. Then write answers to the following questions based on your reading of the section on [Docker compose files](https://jkropko.github.io/surfing-the-data-pipeline/docker.html#docker-compose-files) in the textbook:\n",
    "\n",
    "1. How many containers are being launched when we execute this compose.yaml file?\n",
    "- 3\n",
    "\n",
    "2. What does each of the containers do?\n",
    "- mysql: runs MySQL database\n",
    "- postgres: runs PostgreSQL server; has tables/rows/columns but with extra features\n",
    "- mongo: runs Mongo database to store info as JSON not tables\n",
    "\n",
    "3. Docker is building these containers from Docker image files that it downloads from a website. On what website are these Docker images stored? (You can provide the general name of this website, no need for the specific URLs)\n",
    "- Docker hub\n",
    "\n",
    "[4 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b\n",
    "If you haven't yet done so, install Docker Desktop on your computer. Go to https://www.docker.com/products/docker-desktop/ and click on the Download button, making sure the operating system listed matches the operating system of your computer (for Apple users, make sure you get the correct version for your computer's processing chip. Click the Apple icon in the top-left corner of your screen, select \"About This Mac\", and see whether Intel or Apple M1 is listed under Chip).\n",
    "\n",
    "Once Docker Desktop is installed, find the Docker Desktop program on your computer and run it.\n",
    "\n",
    "To confirm that Docker Desktop is running, open a terminal and type `docker version`. Copy-and-paste what you see on the screen into your notebook.\n",
    "\n",
    "If your terminal hangs without displaying anything, that likely means that Docker Desktop is not running or not properly installed. Double check that the Docker Desktop is running. If your terminal is hanging, you can press Control + C to regain access to the command prompt.\n",
    "\n",
    "- (base) tiandrathreat@MacBook-Air Module 6 % docker version\n",
    "Client:\n",
    " Version:           28.4.0\n",
    " API version:       1.51\n",
    " Go version:        go1.24.7\n",
    " Git commit:        d8eb465\n",
    " Built:             Wed Sep  3 20:56:26 2025\n",
    " OS/Arch:           darwin/arm64\n",
    " Context:           desktop-linux\n",
    "\n",
    "- Server: Docker Desktop 4.47.0 (206054)\n",
    " Engine:\n",
    "  Version:          28.4.0\n",
    "  API version:      1.51 (minimum version 1.24)\n",
    "  Go version:       go1.24.7\n",
    "  Git commit:       249d679\n",
    "  Built:            Wed Sep  3 20:58:53 2025\n",
    "  OS/Arch:          linux/arm64\n",
    "  Experimental:     false\n",
    " containerd:\n",
    "  Version:          1.7.27\n",
    "  GitCommit:        05044ec0a9a75232cad458027ca83437aae3f4da\n",
    " runc:\n",
    "  Version:          1.2.5\n",
    "  GitCommit:        v1.2.5-0-g59923ef\n",
    " docker-init:\n",
    "  Version:          0.19.0\n",
    "  GitCommit:        de40ad0\n",
    "(base) tiandrathreat@MacBook-Air Module 6 % \n",
    "\n",
    "[4 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c\n",
    "Inside your \"ds6001databases\" folder, create a file named `.env`. Inside the .env file you need to choose passwords for the MySQL, PostgreSQL, and MongoDB databases, so type\n",
    "```\n",
    "MYSQL_ROOT_PASSWORD=password1\n",
    "POSTGRES_PASSWORD=password2\n",
    "MONGO_INITDB_ROOT_PASSWORD=password3\n",
    "MONGO_INITDB_ROOT_USERNAME=mongo\n",
    "mongo_init_db = mongodb\n",
    "MYSQL_DATABASE=mysql\n",
    "```\n",
    "Change the passwords on the first three lines to whatever you want, but DON'T USE THE @ SYMBOL as that will cause problems. Leave the fourth, fifth, and sixth lines alone, as well as the names of each environmental variable.\n",
    "\n",
    "Then run this line of Python code:\n",
    "```\n",
    "dotenv.load_dotenv()\n",
    "```\n",
    "If the .env file has successfully been created and saved in the correct location, the output of this code will be `True`. If `False`, double check the name and location of your file.\n",
    "\n",
    "[4 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part d\n",
    "In the terminal, make sure you are in the \"ds6001databases\" folder (you can check by typing `pwd`. If not, then use `cd` to navigate to the \"ds6001databases\" folder). Then type\n",
    "```\n",
    "docker compose up\n",
    "```\n",
    "This command launches all of the containers for running each of the database systems. If successful, you will see a long stream of output with messages that begin `ds6001databases-postgres-1`, `ds6001databases-mysql-1`, and `ds6001databases-mongo-1`. Copy and paste below the first six lines of this output. (Apologies for all the scrolling you might have to do!)\n",
    "\n",
    "If you receive an error message or if the terminal hangs without displaying output, you will likely need to double-check that parts a, b, and c can still be completed successfully, and that you are working within the \"ds6001databases\" folder via the terminal.\n",
    "\n",
    "- Done with no errors\n",
    "\n",
    "[4 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part e\n",
    "To confirm that the databases are running on your system, run the following Python code. \n",
    "```\n",
    "# 1. Load needed environment variables\n",
    "dotenv.load_dotenv()\n",
    "POSTGRES_PASSWORD = os.getenv('POSTGRES_PASSWORD')\n",
    "MONGO_INITDB_ROOT_USERNAME = os.getenv('MONGO_INITDB_ROOT_USERNAME')\n",
    "MONGO_INITDB_ROOT_PASSWORD = os.getenv('MONGO_INITDB_ROOT_PASSWORD')\n",
    "MYSQL_ROOT_PASSWORD = os.getenv('MYSQL_ROOT_PASSWORD')\n",
    "\n",
    "# 2. Test MySQL\n",
    "dbserver = mysql.connector.connect(\n",
    "    user='root', \n",
    "    password=MYSQL_ROOT_PASSWORD, \n",
    "    host='localhost',\n",
    "    port='3306',\n",
    "    #auth_plugin='mysql_native_password'\n",
    ")\n",
    "\n",
    "# 3. Test PostgreSQL\n",
    "dbserver = psycopg.connect(\n",
    "    user='postgres', \n",
    "    password=POSTGRES_PASSWORD, \n",
    "    host='localhost',\n",
    "    port = '5432'\n",
    ")\n",
    "dbserver.autocommit = True\n",
    "\n",
    "# 4. Test MongoDB\n",
    "myclient = pymongo.MongoClient(f\"mongodb://{MONGO_INITDB_ROOT_USERNAME}:{MONGO_INITDB_ROOT_PASSWORD}@localhost:27017/\")\n",
    "myclient.list_databases()\n",
    "```\n",
    "\n",
    "If this code runs without error, you will see output that looks something like\n",
    "```\n",
    "<pymongo.synchronous.command_cursor.CommandCursor at 0x177d8b380>\n",
    "```\n",
    "and you are all set for working with these databases using Python.\n",
    "\n",
    "If you do see an error, then you will need to troubleshoot. A few things to try:\n",
    "\n",
    "* Double check that your work for parts a, b, c, and d can still run successfully.\n",
    "\n",
    "* Read the error to determine which of the databases is causing the error, to narrow the problem down. Comment-out or delete this part of the code to see if the other databases are working.\n",
    "\n",
    "* Check the output in the terminal where you issued the `docker compose up` command. Scroll up and down, and look for any lines that write that a container has exited.\n",
    "\n",
    "* Try a reset of the containers by pressing Control+C in the terminal displaying Docker output, then `docker compose down`, then `docker compose up` again. Then try running the Python code again.\n",
    "\n",
    "* Sometimes (especially if you've changed the database passwords at some point) incorrect passwords get saved in the Docker volumes and do not change when you make a change to your .env file. To fix this, press Control+C in the terminal displaying Docker output, then `docker compose down`. Then open the Docker Desktop, click on volumes, and find and delete the volumes associated with the \"ds6001databases\" containers. Then return to the terminal,type `docker compose up`, and try again.\n",
    "\n",
    "* If all else fails, you are far from the only one to hit a roadblock at this stage. Send me or one of the TAs a message and we can help.\n",
    "\n",
    "[4 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.synchronous.command_cursor.CommandCursor at 0x137c7ee40>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Load needed environment variables\n",
    "dotenv.load_dotenv()\n",
    "POSTGRES_PASSWORD = os.getenv('POSTGRES_PASSWORD')\n",
    "MONGO_INITDB_ROOT_USERNAME = os.getenv('MONGO_INITDB_ROOT_USERNAME')\n",
    "MONGO_INITDB_ROOT_PASSWORD = os.getenv('MONGO_INITDB_ROOT_PASSWORD')\n",
    "MYSQL_ROOT_PASSWORD = os.getenv('MYSQL_ROOT_PASSWORD')\n",
    "\n",
    "# 2. Test MySQL\n",
    "dbserver = mysql.connector.connect(\n",
    "    user='root', \n",
    "    password=MYSQL_ROOT_PASSWORD, \n",
    "    host='localhost',\n",
    "    port='3306',\n",
    "    #auth_plugin='mysql_native_password'\n",
    ")\n",
    "\n",
    "# 3. Test PostgreSQL\n",
    "dbserver = psycopg.connect(\n",
    "    user='postgres', \n",
    "    password=POSTGRES_PASSWORD, \n",
    "    host='localhost',\n",
    "    port = '5432'\n",
    ")\n",
    "dbserver.autocommit = True\n",
    "\n",
    "# 4. Test MongoDB\n",
    "myclient = pymongo.MongoClient(f\"mongodb://{MONGO_INITDB_ROOT_USERNAME}:{MONGO_INITDB_ROOT_PASSWORD}@localhost:27017/\")\n",
    "myclient.list_databases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 \n",
    "For this problem, I created fake data that represents patients who saw a doctor and received one or more prescriptions. Your goal in this problem is to reorganize the data into a database schema that conforms to E. F. Codd's 3rd normal form, and to document the resulting relational database using an ER diagram.\n",
    "\n",
    "Wrapping your head around the normal form rules and building and documenting a database schema can be very difficult. First, grant yourself a lot of grace and give yourself plenty of time to take on this problem. Mastering database schema and documentation are central skills that distinguish a data engineer from a data scientist or analyst, so it is worth taking time to think about this task.\n",
    "\n",
    "The data are listed on the following Google sheet: https://docs.google.com/spreadsheets/d/11wUk5dg39lmI-_O0SQ0M5_5VGgzuGJKvSyMq4Tyxc9M/edit?usp=sharing. For this problem you will be manipulating the data. I only included 10 rows to enable you to use any method you like for working with the data, including point-and-click data manipulation on Google sheets or Excel. Or you can use `pandas` if you like. We are emphasizing the product over the process for now. \n",
    "\n",
    "There are two tabs on this spreadsheet. The first tab contains the data and the second tab contains column descriptions and notes. These notes (some of which are unrealistic, but needed for this exercise) are:\n",
    "\n",
    "* No two patients have the same name and date of birth, so `patient_name` and `date_of_birth` together uniquely identify a patient.\n",
    "\n",
    "* No two drugs share the same brand name, so `prescribed_drug` uniquely identifies the drug.\n",
    "\n",
    "* The data contain one row for every patient/prescription combination, and no patient receives more than one prescription for the same drug. So `patient_name`, `date_of_birth`, and `prescribed_drug` comprise a primary key in the original data.\n",
    "\n",
    "* No two physicians in the data share the same name, so `prescribing_physician` uniquely identifies physicians.\n",
    "\n",
    "* Patients do not change their insurance provider.\n",
    "\n",
    "* Every physician works at exactly one hospital, no two hospitals share the same name, and every hospital exists at exactly one location.\n",
    "\n",
    "Some things to keep in mind for this problem:\n",
    "\n",
    "1. Remember that the first rule for 2nd normal form is that the data are in 1st normal form, and the first rule for 3rd normal form is that the data are already in 2nd normal form. So please focus on meeting the rules for 1st, then 2nd, then 3rd normal form *in that order*. \n",
    "\n",
    "2. Some students have never worked with databases before, but some have. If you have seen databases before, you have some ideas about how database schema tend to look. But if you arrange the data to match an idea of how databases often look, you will more than likely over-engineer the database by creating too many tables or unnecessary columns. Instead, focus on the explicit rules for each normal form and stop once those rules are met. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Part a\n",
    "In words, address the following points regarding 1st normal form:\n",
    "\n",
    "* Describe the problem or problems that exist in the original data that prevent it from meeting the requirements for 1st normal form. \n",
    "\n",
    "* Describe a reorganization of the data that does conform to 1st normal form. If this schema has multiple tables, give each table a name that indicates what the rows in the table represent. List the columns that exist in each table, and if the number of rows in a table is different from the number of rows in the original data, explain why. \n",
    "\n",
    "(It will help you to make these edits to the data now, but you won't show your final data until part d)\n",
    "\n",
    "[8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b\n",
    "In words, address the following points regarding 2nd normal form:\n",
    "\n",
    "* Describe the problem or problems that exist in the 1st normal form data that prevent it from meeting the requirements for 2nd normal form. \n",
    "\n",
    "* Describe a reorganization of the data that does conform to 2nd normal form. If this schema has multiple tables, give each table a name that connects to what the rows in the table represent. List the columns that exist in each table, and if the number of rows in a table is different from the number of rows in the original data, explain why. \n",
    "\n",
    "(It will help you to make these edits to the data now, but you won't show your final data until part d)\n",
    "\n",
    "[8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c\n",
    "In words, address the following points regarding 3rd normal form:\n",
    "\n",
    "* Describe the problem or problems that exist in the 2nd normal form data that prevent it from meeting the requirements for 3rd normal form. \n",
    "\n",
    "* Describe a reorganization of the data that does conform to 3rd normal form. If this schema has multiple tables, give each table a name that connects to what the rows in the table represent. List the columns that exist in each table, and if the number of rows in a table is different from the number of rows in the original data, explain why. \n",
    "\n",
    "(It will help you to make these edits to the data now, but you won't show your final data until part d)\n",
    "\n",
    "[8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part d\n",
    "Display all of the tables in their entirety from the version of the data you designed for part (c) to meet the requirements of 3rd normal form. \n",
    "\n",
    "You can use several different methods to display these tables. You can download the tables to CSV format then upload them into your notebook using `pd.read_csv()`, or you can represent them using Markdown table syntax using a tool such as  https://www.tablesgenerator.com/markdown_tables, or you can include them as screenshot images if you want. (Please don't just provide external links however, as that will slow down our grading)\n",
    "\n",
    "[4 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "For this problem, you will be documenting and a building database that contains the entire collected works of Shakespeare.\n",
    "\n",
    "<img src=\"https://image.cagle.com/178551/750/178551.png\" width=\"300\" alt='Shakespeare Twitter comic'>\n",
    "\n",
    "The data were collected by [Catherine Devlin](https://github.com/catherinedevlin/opensourceshakespeare) for https://opensourceshakespeare.org/. The database will have five tables, and they will be brought into your Python environment as `pandas` dataframes in 3rd normal form  by running this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = 'https://github.com/jkropko/DS-6001/raw/master/localdata/'\n",
    "works = pd.read_csv(repo + 'Works.csv')\n",
    "characters = pd.read_csv(repo + 'Characters.csv')\n",
    "chapters = pd.read_csv(repo + 'Chapters.csv')\n",
    "paragraphs = pd.read_csv(repo + 'Paragraphs.csv')\n",
    "\n",
    "# convert column names to lowercase (needed for postgreSQL to work properly)\n",
    "characters.columns = characters.columns.str.lower() \n",
    "chapters.columns = chapters.columns.str.lower()\n",
    "paragraphs.columns = paragraphs.columns.str.lower()\n",
    "works.columns = works.columns.str.lower()\n",
    "\n",
    "# works in the characters tables is a comma separated list. \n",
    "# Break it out into multiple rows in a new table\n",
    "charworks = characters[['charid', 'works']]\n",
    "charworks.loc[:,'works'] = charworks['works'].str.split(',')\n",
    "charworks = charworks.explode('works')\n",
    "charworks = charworks.rename({'works':'workid'}, axis=1)\n",
    "characters = characters.drop('works', axis=1)\n",
    "\n",
    "#Remove empty rows\n",
    "chapters = chapters.query(\"~chapterid.isnull()\")\n",
    "paragraphs = paragraphs.query(\"~paragraphid.isnull()\")\n",
    "charworks = charworks.query(\"~workid.isnull()\")\n",
    "\n",
    "# Add chapterid to paragraphs\n",
    "paragraphs = pd.merge(paragraphs, \n",
    "                      chapters.drop('description', axis=1),\n",
    "                      how='inner', \n",
    "                      on=['workid', 'section', 'chapter'])\n",
    "\n",
    "#Remove unnecessary columns\n",
    "paragraphs = paragraphs.drop(['paragraphtype', 'section', 'chapter'], \n",
    "                             axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The five tables are:\n",
    "\n",
    "**works**: One row per work authored by Shakespeare, with columns:\n",
    "* `workid`: (primary key) a unique ID without spaces or special characters for the work\n",
    "* `title`: the title, such as \"Twelfth Night\"\n",
    "* `longtitle`: a longer title, if there is one, such as \"Twelfth Night, Or What You Will\"\n",
    "* `date`: year of publication\n",
    "* `genretype`: `t` is a tragedy, such as *Romeo and Juliet* and *Hamlet*; `c` is a comedy, such as *A Midsummer Night's Dream* and *As You Like It*; `h` is a history, such as *Henry V* and *Richard III*; `s` refers to Shakespeare's sonnets; `p` is a narrative (non-sonnet) poem, such as *Venus and Adonis* and *Passionate Pilgrim*\n",
    "* `notes`: Column for notes from the database maintainer, currently all `NaN`\n",
    "* `source`: whether the text was originally downloaded from the [Moby Project lexicon](https://en.wikipedia.org/wiki/Moby_Project) or [Project Gutenberg](https://www.gutenberg.org/). \n",
    "* `totalwords`: Total words in the work\n",
    "* `totalparagraphs`: Total number of lines of dialogue for plays, or stanzas for poems\n",
    "\n",
    "**characters**: One row per character that appears in at least one work by Shakespeare. Some characters, such as Antony or Henry IV, appear in multiple works. Columns:\n",
    "* `charid`: (primary key) a unique ID for a character\n",
    "* `charname`: character's name (some characters are different but have the same name, such as the First Musician in Othello and the First Musician in Romeo and Juliet). For poems, the character is \"Poet\" \n",
    "* `abbrev`: an abbreviation of the character's name, if needed for reference to some other analyses\n",
    "* `description`: a longer description of who the character is, if available\n",
    "* `speechcount`: number of lines of dialogue delivered by the character throughout the works the character appears in\n",
    "\n",
    "**chapters**: One row for every unique scene in a play, or for every distinct poem in a collection of poems. Columns:\n",
    "* `workid`: a unique ID without spaces or special characters for the work\n",
    "* `chapterid`: (primary key) a unique ID for the scene/poem\n",
    "* `section`: the scene/poem number\n",
    "* `chapter`: the act number, if available\n",
    "* `description`: short description of where the scene takes place, for plays\n",
    "\n",
    "**paragraphs**: One row for every line of dialogue that appears in a Shakespeare play, or for every distinct poem in a collection of poems. Columns:\n",
    "* `workid`: a unique ID without spaces or special characters for the work\n",
    "* `paragraphid`: (primary key) a unique ID for the line of dialogue/poem\n",
    "* `paragraphnum`: the position of the paragraph within the ordered list of paragraphs within a chapter\n",
    "* `charid`: the unique ID of the character delivering the line of dialogue/poem\n",
    "* `plaintext`: the text of the dialogue/poem\n",
    "* `phonetictext`: the text of the dialogue/poem in phonetic text, useful for training computers to generate audio of this spoken text\n",
    "* `stemtext`: the stems of the words in the text, useful for text analyses such as sentiment analysis\n",
    "* `charcount`: number of characters in the line\n",
    "* `wordcount`: number of words in the line\n",
    "* `chapterid`: unique ID for the scene/poem\n",
    "\n",
    "**charworks**: One row for every unique combination of character and play. Most characters appear once, but some (such as Antony or Henry IV) appear multiple times. Columns\n",
    "* `charid`: (primary key) unique ID for the character\n",
    "* `workid`: (primary key) unique ID for the work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a\n",
    "\n",
    "Please refer to the [textbook's discussion of ER diagrams](https://jkropko.github.io/surfing-the-data-pipeline/ch6.html#entity-relationship-diagrams) as you complete this problem.\n",
    "\n",
    "For each of the following pairs of tables, write\n",
    "1. Which column or columns the tables should be joined on\n",
    "2. A description of your reasoning, in plain language, for whether each table matches to one or many rows in the other\n",
    "3. Whether there is a one-to-one, one-to-many, many-to-one, or many-to-many relationship between the tables\n",
    "4. Write the database markup language (DBML) code to represent this kind of a relationship. \n",
    "\n",
    "If we are connecting two tables named `table_a` and `table_b` on a column named `joincolumn` in each table, then the DBML syntax is \n",
    "\n",
    "* `Ref: table_a.joincolumn - table_b.joincolumn` if the relationship from `table_a` to `table_b` is one-to-one\n",
    "\n",
    "* `Ref: table_a.joincolumn < table_b.joincolumn` if the relationship from `table_a` to `table_b` is one-to-many\n",
    "\n",
    "* `Ref: table_a.joincolumn > table_b.joincolumn` if the relationship from `table_a` to `table_b` is many-to-one\n",
    "\n",
    "* `Ref: table_a.joincolumn <> table_b.joincolumn` if the relationship from `table_a` to `table_b` is many-to-many\n",
    "\n",
    "If there are multiple columns that must be joined on, provide the DBML code for each of these columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part a, problem i\n",
    "**charworks** and **works** [2 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part a, problem ii\n",
    "**characters** and **charworks** [2 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part a, problem iii\n",
    "**works** and **paragraphs** [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part a, problem iv\n",
    "**chapters** and **works** [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part a, problem v\n",
    "**paragraphs** and **chapters** [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part a, problem vi\n",
    "**characters** and **paragraphs** (count groups that might say a line in unison, like Chorus or Witches, as one character) [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b\n",
    "\n",
    "Please find the \"Using dbdiagram.io and dbdocs.io to Document Your Database\" page on Canvas. It is available under Modules and \"Getting Started with the Tools We'll Be Using\". Read the page and watch the video about using dbdocs.io and dbdiagram.io to document a relational database with an ER diagram.\n",
    "\n",
    "Use https://dbdiagram.io to create an ER diagram for the five Shakespeare data tables, then publish this diagram to a stable URL by pressing the \"Publish to dbdocs\" button. Paste a link here to your ER diagram on DBdocs.io. Then paste your DBML code from dbdiagrams.io into the box below.\n",
    "\n",
    "A few notes:\n",
    "\n",
    "You can use the `pandas_df_to_dbml()` function, defined below, to generate database markup language (DBML) code for each table that also lists each column's data type, then paste this code into dbdiagram.io (use `print()` around the output to see the correct formatting):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_df_to_dbml(df: pd.DataFrame, table_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts a pandas DataFrame to a DBML string.\n",
    "\n",
    "    Args:\n",
    "        df: The pandas DataFrame to convert.\n",
    "        table_name: The name of the table in the DBML schema.\n",
    "\n",
    "    Returns:\n",
    "        A DBML string representing the DataFrame schema.\n",
    "    \"\"\"\n",
    "\n",
    "    dbml_string = f\"Table {table_name} {{\\n\"\n",
    "\n",
    "    for column_name, column_type in df.dtypes.items():\n",
    "        dbml_type = map_pandas_dtype_to_dbml_type(column_type)\n",
    "        dbml_string += f\"  {column_name} {dbml_type}\\n\"\n",
    "\n",
    "    dbml_string += \"}\\n\"\n",
    "    return dbml_string\n",
    "\n",
    "def map_pandas_dtype_to_dbml_type(dtype) -> str:\n",
    "    \"\"\"Maps a pandas dtype to a DBML type.\"\"\"\n",
    "    dtype_name = str(dtype)\n",
    "    if \"int\" in dtype_name:\n",
    "      return \"int\"\n",
    "    if \"float\" in dtype_name:\n",
    "      return \"float\"\n",
    "    if \"datetime\" in dtype_name:\n",
    "        return \"datetime\"\n",
    "    return \"varchar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the syntax [pk] after a column name and data type to designate the columns that are primary keys in each table. In this case the primary keys are \n",
    "\n",
    "| Table      | Primary Key column(s) |\n",
    "|------------|-----------------------|\n",
    "| works      | workid                |\n",
    "| characters | charid                |\n",
    "| paragraphs | paragraphid           |\n",
    "| chapters   | chapterid             |\n",
    "| charworks  | charid, workid        |\n",
    "\n",
    "To draw the lines linking one table to another, include the `Ref:` syntax that you wrote for your answers to part (a).\n",
    "\n",
    "[4 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Make sure you get the .ipynb file for this lab from the module 6 page on Canvas\n",
    "\n",
    "Then when you double-click this box, you'll see three backticks before and after this text. Leave those alone\n",
    " \n",
    "Type your code here, between the backticks\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "For this problem, you will use the five dataframes that comprise the Shakespeare database you used in problem 3 to initialize local databases using SQlite, MySQL, and PostgreSQL. \n",
    "\n",
    "All of the methods for initializing and connecting to databases in Python are listed in the [textbook](https://jkropko.github.io/surfing-the-data-pipeline/ch6.html#working-with-databases-in-python). Note that there are differences in code that depend on whether you are using SQlite, MySQL, or PostgreSQL. \n",
    "\n",
    "For creating databases on the MySQL and PostgreSQL servers, you will connect directly to the database server and use the `.cursor()` method to interact with it. But once the database exists, please use the `pd.to_sql()` and `pd.read_sql_query()` methods from `pandas` and the `create_engine()` method from `sqlalchemy` (and not the `.cursor()` approach) to add data to or retrieve data from the database. \n",
    "\n",
    "Before attempting this problem, please make sure your work for problem 1 runs without error, which will ensure your docker containers for MySQL and PostgreSQL are ready to use (SQlite does not require external software and can run without Docker)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a\n",
    "Initialize a new Shakespeare database using SQlite via the `sqlite3` package. Next use the `pd.to_sql()` method to add the five Shakespeare dataframes to this database. Then, to prove that this worked, issue the following SQL query to the database which should display a dataframe listing all characters from Shakespeare's plays with more than 200 lines of dialogue (and the Poet too):\n",
    "```\n",
    "SELECT charname, description, speechcount\n",
    "FROM characters\n",
    "WHERE speechcount > 200\n",
    "```\n",
    "Finally, after running the query, use the `.commit()` and `.close()` methods on the Python variable containing the database connection to save your changes and prevent further changes.\n",
    "\n",
    "[8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b\n",
    "Use the `dotenv` package to import your MySQL database password into your Python environment (don't expose this password in your code). Then use this password to initialize a new Shakespeare database using MySQL and the `mysql.connector` package. \n",
    "\n",
    "Next, use the `create_engine()` method from `sqlalchemy` and the `pd.to_sql()` method to add the five Shakespeare dataframes to this database. Then, to prove that this worked, use the `pd.read_sql_query()` method to issue the following SQL query to the database\n",
    "```\n",
    "SELECT charname, description, speechcount\n",
    "FROM characters\n",
    "WHERE speechcount > 200\n",
    "```\n",
    "Finally, after running the query, use the `.commit()` and `.close()` methods on the Python variable containing the database connection to save your changes and prevent further changes.\n",
    "\n",
    "[8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part c\n",
    "Use the `dotenv` package to import your PostgreSQL database password into your Python environment (don't expose this password in your code). Then use this password to initialize a new Shakespeare database using PostgreSQL and the `psycopg` package. (For PostgreSQL, you will need to set the `.autocommit` attribute of Python variable that connects to the PostgreSQL server to `True`, or you will receive cryptic errors.)\n",
    "\n",
    "Next, use the `create_engine()` method from `sqlalchemy` and the `pd.to_sql()` method to add the five Shakespeare dataframes to this database (don't worry if a negative number is displayed. This is a known issue with PostgreSQL in Python but is not indicative of a problem). Then, to prove that this worked, use the `pd.read_sql_query()` method to issue the following SQL query to the database\n",
    "```\n",
    "SELECT charname, description, speechcount\n",
    "FROM characters\n",
    "WHERE speechcount > 200\n",
    "```\n",
    "Finally, after running the query, use the `.commit()` and `.close()` methods on the Python variable containing the database connection to save your changes and prevent further changes.\n",
    "\n",
    "(If you see an error like `ObjectInUse: database \"shakespeare\" is being accessed by other users DETAIL:  There is 1 other session using the database.`, go to the terminal, type Control+C, then `docker compose down`, then `docker compose up`, then try again.)\n",
    "\n",
    "[8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5\n",
    "[Colin Mitchell](http://muffinlabs.com/) is a web-developer and artist who has a bunch of cool projects that play with what data can do on the internet. One of his projects is [Today in History](https://history.muffinlabs.com/), which provides an API to access all the Wikipedia pages for historical events that happened on this day in JSON format. The records in this JSON are stored in the `['data']['events']` path. Here's the first listing for today (the day I ran this code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'year': '2457 BC',\n",
       " 'text': \"Gaecheonjeol, Hwanung (환웅) purportedly descended from heaven. South Korea's National Foundation Day.\",\n",
       " 'html': '2457 BC - 2457 BC - <a href=\"https://wikipedia.org/wiki/Gaecheonjeol\" class=\"mw-redirect\" title=\"Gaecheonjeol\">Gaecheonjeol</a>, Hwanung (환웅) purportedly descended from heaven. South Korea\\'s National Foundation Day.',\n",
       " 'no_year_html': '2457 BC - <a href=\"https://wikipedia.org/wiki/Gaecheonjeol\" class=\"mw-redirect\" title=\"Gaecheonjeol\">Gaecheonjeol</a>, Hwanung (환웅) purportedly descended from heaven. South Korea\\'s National Foundation Day.',\n",
       " 'links': [{'title': 'Gaecheonjeol',\n",
       "   'link': 'https://wikipedia.org/wiki/Gaecheonjeol'}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = requests.get(\"https://history.muffinlabs.com/date\")\n",
    "history_json = json.loads(history.text)\n",
    "events = history_json['data']['Events']\n",
    "events[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the count of total events for the day I ran this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, you will use MongoDB and the `pymongo` package to create a local document store NoSQL database containing these historical events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a\n",
    "First, check that your work for problem 1 is still running without error. If so, then you have a MongoDB server running inside a Docker container. \n",
    "\n",
    "Use `pymongo` to connect to the local MongoDB client, create a database named \"history\" and a collection within that database named \"today\". Because you will probably be running this code several times as you work, debugging as you go along, it's useful write code that deletes any existing \"today\" collections before creating a new \"today\" collection.\n",
    "\n",
    "[4 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b\n",
    "Insert all of the records in `events` into this collection. [4 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c\n",
    "Issue the following query to find all of the records whose text contain the word \"Virginia\":\n",
    "```\n",
    "query = {\n",
    "    \"text\":{\n",
    "        \"$regex\": 'England'\n",
    "    }\n",
    "}\n",
    "```\n",
    "If there are no results that contain the word \"England\", choose a different word like \"China\" or \"war\". Display the count of the number of documents that match this query, display the output of the query, and generate a JSON formatted variable containing the output. [4 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6\n",
    "Once you are finished working with databases, clear up the space on your computer by going to the terminal that you used to launch the Docker containers, press CONTROL + C on your keyboard to stop the containers, then type `docker compose down` to disconnect the volumes and networks. It's a good idea to make a practice out of doing these steps when you finish working with databases.\n",
    "\n",
    "This problem isn't graded, and no need to write anything. But please do this anyway."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds6001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
